# -*- coding: utf-8 -*-
"""Bike_Sharing_Demand_Prediction_Capstone_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKCQngnHSSSyFrtlC7zvSivyq3PBcKyr

# <b><u> Project Title : Seoul Bike Sharing Demand Prediction </u></b>

## <b> Problem Description </b>

### Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.

## <b> Data Description </b>

### <b> The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.</b>


### <b>Attribute Information: </b>

* ### Date : year-month-day
* ### Rented Bike count - Count of bikes rented at each hour
* ### Hour - Hour of he day
* ### Temperature-Temperature in Celsius
* ### Humidity - %
* ### Windspeed - m/s
* ### Visibility - 10m
* ### Dew point temperature - Celsius
* ### Solar radiation - MJ/m2
* ### Rainfall - mm
* ### Snowfall - cm
* ### Seasons - Winter, Spring, Summer, Autumn
* ### Holiday - Holiday/No holiday
* ### Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing required libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from statsmodels.stats.outliers_influence import variance_inflation_factor
from xgboost import XGBRegressor

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

#df1 = pd.read_csv("/content/SeoulBikeData.csv",encoding = 'latin',parse_dates=['Date'])

df = pd.read_csv("/content/drive/MyDrive/capstone project/bike demand/SeoulBikeData.csv",encoding = 'unicode_escape')

df.head(10).T

df.tail(10).T

len(df)

df.shape

df.dtypes

df.isnull().sum()

df.info()

df.describe()

"""## *There is no missing values*"""

df['Holiday'].value_counts()

df['Functioning Day'].value_counts()

df.columns

# Renaming Columns
df.rename(columns={'Date': 'date', 'Rented Bike Count': 'bike_count', 'Hour': 'hour',
                   'Temperature(°C)': 'temp', 'Humidity(%)': 'humidity', 'Wind speed (m/s)': 'wind',
                   'Visibility (10m)': 'visibility', 'Dew point temperature(°C)': 'dew_temp',
                   'Solar Radiation (MJ/m2)': 'sunlight', 'Rainfall(mm)': 'rain', 'Snowfall (cm)': 'snow',
                   'Seasons': 'season', 'Holiday': 'holiday', 'Functioning Day': 'functioning_day'}, inplace=True)

"""### convertion date time format"""

df['date']=pd.to_datetime(df['date'])

# Extracting new features from date and hour column
df['weekend'] = df['date'].apply(lambda x : 1 if (x.day_name()=='Saturday' or x.day_name()=='Sunday') else 0 )
df['timeshift'] = df['hour'].apply(lambda x: 'night' if 0<=x<=6 else ('day' if 7<=x<=16 else 'evening'))
# Dropping the Date column
df.drop(columns=['date'], inplace = True)

df.info()

# Checking unique value
df.apply(lambda x: len(x.unique()))

#EXCLUDE'OBJECT'
numeric_features= df.select_dtypes(exclude='object')
numeric_features

df.corr()



#checking correlation using heatmap
plt.figure(figsize=(15,10))
sns.heatmap(df.corr(),cmap='PuBu',annot=True)

# Multicollinearity

from statsmodels.stats.outliers_influence import variance_inflation_factor
def calc_vif(X):

   # Calculating VIF
   vif = pd.DataFrame()
   vif["variables"] = X.columns
   vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

   return(vif)

calc_vif(df[[i for i in df.describe().columns if i not in ['bike_count','dew_temp'] ]])

# ploting Regression plot of each columns of dataset v/s bike_count columns

for col in numeric_features[:]:
  if col == 'bike_count':
    pass
  else:
    sns.regplot(x=df[col],y=df["bike_count"],line_kws={"color": "red"})

  plt.show()

#INCLIDE'OBJECT'
categorical_features= df.select_dtypes(include='object')
categorical_features

for col in categorical_features:
  plt.figure(figsize=(10,8))
  sns.boxplot(x=df[col],y=df['bike_count'])

"""CONCLUSION
1. IN TIME SHIFT DAY AND NIGHT HAVE LESS BIKE DEMAND AS COMPARE TO EVENING
2. ON FUNCTIONAL DAY HAVE GREAT BIKE DEMAND
3. IN WINTER BIKE DEMAND HAVE LESS
"""

#ploting cat plot for more info
sns.stripplot(x='season',y='bike_count',data=df)

avg_rent_hrs = df.groupby('hour')['bike_count'].mean()

avg_rent_hrs

#ploting line graph
plt.figure(figsize=(20,4))
a=avg_rent_hrs.plot(legend=True,marker='o',title="Average Bikes Rented Per Hr")
a.set_xticks(range(len(avg_rent_hrs)))
plt.show()

"""CONCLUSION
PEOPLE PREFERED RENTED BIKE DURING 8:00 AM TO 9:00 PM

#one hot encoding
"""

dummy_categorical_feature= pd.get_dummies(categorical_features,drop_first=True)
dummy_categorical_feature

final_df= pd.concat([dummy_categorical_feature,numeric_features],axis=1)
final_df.head()

final_df.describe().T

df.columns

feature_list=["hour","holiday",'rain','snow','weekend']
for feature in feature_list:
  plt.figure(figsize=(10,8),dpi=200)
  sns.catplot(x=feature,y='bike_count',data=df)
  plt.show()

"""Conclusion:

**From hour v/s rented bike**
- we can clearly see there is high demand of Rented bike between the office hours.

**From working-nonworking v/s rented bike**

- As cleared from 2nd plot working days has comparatively high demand of rented bike as compared to non working day

**From Rainfall v/s rented bike**

- we can see that if Rainfall increase demand of Rented Bike Decreases


**From Snowfall v/s rented bike**

- we can see that if Snowfall increase demand of Rented Bike Decreases
"""

plt.figure(figsize=(10,8))
sns.distplot(df['bike_count'])

"""square_root transformation"""

plt.figure(figsize=(10,8))
sns.distplot(np.sqrt(df['bike_count']))

"""# **CREATING FUNCTION**"""

#creating list of matrix to store the evaluation matrix of all model
mean_sq_error=[]
root_mean_sq_error=[]
r2_list=[]
adj_r2_list=[]

"""# creating function for linear models"""

# making a function to train and evaluate linear model
def train_linear_model (model,X_train,y_train,X_test,y_test):

#training the model
  model.fit(X_train,y_train)

  #predicting the values
  pred = model.predict(X_test)
  import math
  from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score

  print('\n****************  |Matrix|  ****************\n')

  #finding mean_squared_error
  MSE  = mean_squared_error(y_test**2,pred**2)
  print("MSE :" , MSE)

  #finding root mean squared error
  RMSE = np.sqrt(MSE)
  print("RMSE :" ,RMSE)

  #finding the r2 score
  r2 = r2_score(y_test**2,pred**2)
  print("R2 :" ,r2)
  #finding the adjusted r2 score
  adj_r2=1-(1-r2_score(y_test**2,pred**2))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))
  print("Adj R2 : ",adj_r2)

  print('\n****************  |Matrix|  ****************\n')


#appending metrice to list
  mean_sq_error.append(MSE)
  root_mean_sq_error.append(RMSE)
  r2_list.append(r2)
  adj_r2_list.append(adj_r2)

  # ploting for actual and predicted values
  print('\
    ***********************************************\
   |Graph|\
    ***********************************************\n')
  plt.figure(figsize=(20,10))
  plt.plot(np.array(y_test[:100]))
  plt.plot(pred[:100])
  plt.legend(["ACTUAL","PREDICTED"],prop={'size': 20})
  plt.show()

"""## creating function for decimal tree models"""

def dac_model(model,X_train,y_train,X_test,y_test):

    # Fit Model
    model.fit(X_train,y_train)

    # Get Metrics

    preds = model.predict(X_test)

    import math
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    from sklearn.metrics import r2_score
    print('\n================Evalution Matrix=========================\n')
    MSE  = mean_squared_error(y_test,preds)
    print("MSE :" , MSE)

    RMSE = np.sqrt(MSE)
    print("RMSE :" ,RMSE)
    r2 = r2_score(y_test,preds)
    print("R2 :" ,r2)
    adj_r2=1-(1-r2_score(y_test,preds))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))
    print("Adjusted R2 : ",adj_r2)

    train_class_preds = linear_model.predict(X_train)

    print('\n================Evalution Matrix=========================\n')

    mean_sq_error.append(MSE)
    root_mean_sq_error.append(RMSE)
    r2_list.append(r2)
    adj_r2_list.append(adj_r2)

    #ploting
    print('\
    =========================================================\
    Evalution Graph\
    ===================================================\n')
    plt.figure(figsize=(20,10))
    plt.plot(np.array(y_test[:100]))
    plt.plot(preds[:100])
    plt.legend(["ACTUAL","PREDICTED"],prop={'size': 20})
    plt.show()

def get_features_importance (optimal_model,X_train):
  '''
  shows the graph of feature importance
  '''
  features = X_train.columns
  importances = optimal_model.feature_importances_
  indices = np.argsort(importances)

  plt.figure(figsize=(15,10))
  plt.title('Feature Importance')
  plt.barh(range(len(indices)), importances[indices], color='red', align='center')
  plt.yticks(range(len(indices)), [features[i] for i in indices])
  plt.xlabel('Relative Importance')

  plt.show()

"""# Train | Test Split for LR"""

# Train - Spliting  data
X=final_df.drop(['bike_count'],axis=1)

X.shape

y=np.sqrt(final_df['bike_count'])

#spliting

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

#scaling data
from sklearn.preprocessing import StandardScaler
std = StandardScaler()
X_train = std.fit_transform(X_train)
X_test = std.transform(X_test)

X_train

"""# linear regression"""

from sklearn.linear_model import LinearRegression

#linear regression model

linear_model= LinearRegression()

train_linear_model(linear_model, X_train, y_train, X_test, y_test)

model=LinearRegression().fit(X_train, y_train)
preds = model.predict(X_test)

plt.figure(figsize=(16,10))
plt.plot((preds)**2)
plt.plot(np.array((y_test)**2))
plt.legend(["Predicted","Actual"])
plt.xlabel('No of Test Data')
plt.show()

sns.relplot(x=preds ,y=y_test,color='red')
plt.title('Predicted vs Actual')
plt.ylabel('Predicted values')
plt.xlabel('Actual Values')

"""# LASSO REGRESSIONS"""

from sklearn.linear_model import Lasso

# finding the best parameters for lasso by gridsearchcv
from sklearn.model_selection import GridSearchCV
lasso_model = Lasso()
#setting the parameters of the lasso model
parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]}
lasso_grid = GridSearchCV(lasso_model, parameters, scoring='neg_mean_squared_error', cv=5)

# calling train_linear_model to train,fit and evalution of lasso model
train_linear_model(lasso_grid,X_train,y_train,X_test,y_test)

"""##Ridge Regression"""

from sklearn.linear_model import Ridge
ridge = Ridge()

# finding the best parameters for ridge model by gridsearchcv

parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,60,100,0.5,1.5,1.6,1.7,1.8,1.9]}
ridge_grid = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)

# calling train_linear_model to train,fit and evalution of ridge model

train_linear_model(ridge_grid,X_train,y_train,X_test,y_test)



"""# Decision Tree Regression

# Train | Test split Data For Tree based methods
"""

#creatin X for independent variable
X=final_df.drop(['bike_count'],axis=1)

#creating y for dependent variable
y=final_df['bike_count']

#spliting X and y to train and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

from sklearn.tree import DecisionTreeRegressor

#creating decision tree model
dt_model = DecisionTreeRegressor()
#calling run_model to train,fit and evalution of decision tree model
cv=dac_model(dt_model,X_train,y_train,X_test,y_test)

features= X_train.columns
importances = dt_model.feature_importances_
indices = np.argsort(importances)

#getting features importance of decision tree
features= X_train.columns
importances = dt_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(15,10))
plt.title('Feature Importance')
plt.barh(range(len(indices)), importances[indices], color='grey', align='edge')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')

plt.show()

"""# Random Forest Regression"""

#creating decision tree model
rf_model = RandomForestRegressor()
#calling run_model to train,fit and evalution of decision tree model
dac_model(rf_model,X_train,y_train,X_test,y_test)

rf_model.score(X_train,y_train)

rf_model.score(X_test,y_test)

y_pred_train=rf_model.predict(X_train)
y_pred=rf_model.predict(X_test)

"""# Gradient Boosting Regression"""

gra_boost=GradientBoostingRegressor()
dac_model(gra_boost,X_train,y_train,X_test,y_test)

"""# Xtreme Gradient Boosting"""

#creating Xgboost model
xgb_model=XGBRegressor()

#creating param dict for gridsearch
n_estimators=[80,100,150]
max_depth=[15,20,30]
params = {'n_estimators':n_estimators,'max_depth':max_depth }

#creating xgb grid model
xgb_grid= GridSearchCV(xgb_model,param_grid=params,verbose=0)

#calling run_model to train,fit and evalution of xgb_grid model
dac_model(xgb_grid,X_train,y_train,X_test,y_test);

#getting best estimator of xgb model given by xgb grid model
optimal_xgb_model=xgb_grid.best_estimator_

#getting best param of rf model given by xgb_grid model
get_features_importance(optimal_xgb_model,X_train)

"""# light GBM"""

import lightgbm as lgb

lgbr=lgb.LGBMRegressor()

# finding the best parameters for XGBRegressor by gridsearchcv for the best result
lgbr_para={'n_estimators': [150,200,250],'max_depth': [7,10,13]}
lgbr_grid=GridSearchCV(estimator=lgbr,param_grid=lgbr_para,cv=5,scoring='neg_mean_squared_error',verbose=5,n_jobs=-1)

#calling run_model to train,fit and evalution of catboost model
dac_model(lgbr_grid,X_train,y_train,X_test,y_test)

"""# creating a list"""

all_model_matrices={'Mean_square_error':mean_sq_error,'Root_Mean_square_error':root_mean_sq_error,'R2':r2_list,'Adjusted_R2':adj_r2_list}

model_name=['Linear','Lasso','Ridge','Decision_Tree','Random_Forest','Gradient_Boosting','Xtreme_GB','light_lgb']

#converting dictionary to dataframe fro easy visual
matrices_df=pd.DataFrame.from_dict(all_model_matrices)
matrices_df

matrices_df=pd.DataFrame.from_dict(all_model_matrices,orient="index",columns=model_name)

matrices_df

"""## **Final conclusion**

1.No overfitting is seen, as we can see the models are performing well with the test data with good results.

2.After performing the various models the lightGBM and Xtreme Gradient Boosting found to be the best model that can be used for the Bike Sharing Demand Prediction since the performance metrics (mse,rmse) shows lower and (r2,adjusted_r2) shows a higher value for the lightGBM and Xtreme Gradient Boosting models !

3.In holiday or non-working days there is demands in rented bikes.

4.People preferred more rented bikes in the morning than the evening.

5.When the rainfall was less, people have booked more bikes except some few cases.

6.The Temperature, Hour & Humidity are the most important features that positively drive the total rented bikes count.

7.We can use either lightGBM or catboost model for the bike rental stations.
"""















